# peer_node.py
import socket
import threading
import time
import json
import sys
import os
import glob
import random
import traceback
import hashlib
from common import *
from file_manager import FileManager

# --- CONFIGURACIÃ“N GLOBAL ---
KNOWN_TRACKERS = [] 

class PeerNode:


    def __init__(self, port, tracker_ip_hint):
        self.my_port = port
        self.my_ip = get_local_ip()
        
        
        print(f"Detectada IP Local: {self.my_ip}")
        
        print("Â¿EstÃ¡s usando Ngrok o IP PÃºblica manual? (Enter para no)")
        public_override = input(">> Escribe tu IP:Puerto pÃºblico (ej. 0.tcp.ngrok.io:12345): ")
        
        if public_override.strip():
            # Si usas Ngrok, esta es tu identidad real ante el mundo
            self.my_id = public_override.strip() 
        else:
            self.my_id = f"{self.my_ip}:{port}"
        
        
        self.folder = f"peer_{port}"
        if not os.path.exists(self.folder): os.makedirs(self.folder)

        self.managers = {} 
        self.managers_lock = threading.Lock()
        
        # EstadÃ­sticas y Seguridad
        self.peer_performance = {} 
        self.download_stats = {}
        self.banned_peers = {} # {peer_id: tiempo_expiracion}
        self.running = True

        print(f"[*] MI IDENTIDAD: {self.my_id}")
        print(f"[*] CARPETA: {self.folder}")
        
        # --- CORRECCIÃ“N MULTI-TRACKER ---
        global KNOWN_TRACKERS
        # Si no hay trackers definidos, usamos el hint. 
        # Si quisieras poner trackers fijos hardcodeados, defÃ­nelos arriba en KNOWN_TRACKERS = [...]
        if not KNOWN_TRACKERS:
            # Por defecto aÃ±adimos el puerto 5000. 
            # Si quieres redundancia automÃ¡tica en la misma IP, podrÃ­as aÃ±adir mÃ¡s puertos.
            KNOWN_TRACKERS.append(("3.151.6.85", 5000))
            KNOWN_TRACKERS.append(("3.137.135.235", 5001))
        # --------
        
        

        self.scan_folder()

        # Iniciar hilos de fondo
        threading.Thread(target=self.start_server, daemon=True).start()
        threading.Thread(target=self.heartbeat_loop, daemon=True).start()

    # ==========================================
    #               HILO DE DESCARGA
    # ==========================================
    def start_download_thread(self, file_hash):
        mgr = self.managers.get(file_hash)
        if not mgr or mgr['fm'].get_progress_percentage() == 100: return
        mgr['downloading'] = True
        threading.Thread(target=self._download_logic, args=(file_hash,), daemon=True).start()

    def _download_logic(self, file_hash):
        mgr = self.managers.get(file_hash)
        fm = mgr['fm']
        self.download_stats = {} 
        
        print(f"[*] Smart Swarm V4 (Turbo) iniciado para: {mgr['filename']}")
        
        # --- CACHÃ‰ DE PEERS (OptimizaciÃ³n CRÃTICA para Windows) ---
        peers = []
        last_tracker_update = 0
        UPDATE_INTERVAL = 10 # Segundos entre consultas al tracker
        
        while fm.get_missing_chunks() and mgr['downloading'] and self.running:
            
            # 1. Actualizar lista de peers (Solo si pasÃ³ el tiempo o estamos solos)
            if (time.time() - last_tracker_update > UPDATE_INTERVAL) or not peers:
                try:
                    new_peers = self.contact_tracker(CMD_ANNOUNCE, file_hash, mgr['trackers'])
                    if new_peers:
                        peers = new_peers
                        last_tracker_update = time.time()
                except Exception as e:
                    # Si falla el tracker, seguimos con los peers que ya conocemos
                    pass
            
            # 2. Filtrar candidatos (excluyÃ©ndome a mÃ­)
            candidates = [p for p in peers if p['id'] != self.my_id]
            
            if not candidates:
                print("[!] Buscando peers...")
                time.sleep(1)
                last_tracker_update = 0 # Forzar actualizaciÃ³n inmediata
                continue

            # 3. Ordenar por rendimiento (Latencia)
            #candidates.sort(key=lambda p: self.peer_performance.get(p['id'], 0.5))
            
            # 3. CAMBIO AQUÃ: Ordenar de MAYOR a MENOR velocidad ---
            candidates.sort(key=lambda p: self.peer_performance.get(p['id'], 0.0), reverse=True)
            
            # --- MEJORA DE COLABORACIÃ“N (Shuffle) ---
            # Si hay varios peers buenos, no uses siempre al #1.
            # Tomamos los 3 mejores y elegimos uno al azar. Esto activa la colaboraciÃ³n.
#            if len(candidates) > 1:
#                top_n = min(len(candidates), 3)
#                best_candidates = candidates[:top_n]
#                random.shuffle(best_candidates) # <--- AquÃ­ estÃ¡ la magia de colaborar
#                candidates = best_candidates + candidates[top_n:]
            
            # 4. Seleccionar quÃ© pieza pedir
            missing = fm.get_missing_chunks()
            if not missing: break
            
            random.shuffle(missing)
            chunk_to_try = missing[0]
            
            chunk_downloaded = False

            # 5. Ronda de intentos
            for p in candidates:
                start_time = time.time()
                
                # Intentar descargar
                result = self.request_chunk(p['id'], chunk_to_try, file_hash, fm)
                
                elapsed = time.time() - start_time
                
                if result == "success":
                    self.update_peer_score(p['id'], elapsed, success=True)
                    self.download_stats[p['id']] = self.download_stats.get(p['id'], 0) + 1
                    
                    # Chismoso: Avisar al tracker (Solo el 10% de las veces para no saturar)
                    if random.random() < 0.1: 
                        threading.Thread(target=self.contact_tracker, 
                                         args=(CMD_ANNOUNCE, file_hash, mgr['trackers']), 
                                         daemon=True).start()
                    chunk_downloaded = True
                    break 

                elif result == "missing":
                    pass 

                elif result == "corrupt":
                    self.update_peer_score(p['id'], elapsed, success=False)

                elif result == "network_error":
                    self.update_peer_score(p['id'], elapsed, success=False)
            
            if not chunk_downloaded: 
                time.sleep(0.1)

        if not fm.get_missing_chunks():
            mgr['downloading'] = False
            self.contact_tracker(CMD_ANNOUNCE, file_hash, mgr['trackers'])
            print(f"\n[â˜…] DESCARGA FINALIZADA: {mgr['filename']}")
            
            # --- BLOQUE DE SEGURIDAD FINAL ---
            print("ğŸ•µï¸ Iniciando validaciÃ³n final post-ensamblaje...")
            full_path = os.path.join(self.folder, mgr['filename'])
            
            if self.verify_integrity(full_path, file_hash):
                print(f"âœ… ARCHIVO GUARDADO Y VERIFICADO: {mgr['filename']}")
            else:
                print("ğŸš¨ ERROR CRÃTICO: La firma digital del archivo no coincide.")
                print("ğŸ—‘ï¸ El archivo estÃ¡ corrupto. Eliminando del disco...")
                
                if os.path.exists(full_path): os.remove(full_path)
                if os.path.exists(full_path + ".progress"): os.remove(full_path + ".progress")
                
                # Borramos el JSON tambiÃ©n para forzar re-check de metadatos
                json_path = os.path.join(self.folder, f"{mgr['filename']}.json")
                if os.path.exists(json_path): os.remove(json_path)

                with self.managers_lock:
                    if file_hash in self.managers: del self.managers[file_hash]

                print("âš ï¸ Archivo y metadatos eliminados por seguridad.")

    def update_peer_score(self, peer_id, time_taken, success):
        """ Sistema basado en VELOCIDAD (MB/s) - VersiÃ³n Mejorada """
        # Obtenemos la velocidad actual (0.0 si es nuevo)
        current_speed = self.peer_performance.get(peer_id, 0.0)
        
        if success:
            # Calculamos velocidad instantÃ¡nea: 1 / tiempo
            # (Si tarda poco, el valor es alto. Ej: 0.1s -> 10 puntos)
            instant_speed = 1.0 / (time_taken + 0.00001) # Evitamos divisiÃ³n por cero
            
            # Promedio mÃ³vil para suavizar picos (70% histÃ³rico, 30% nuevo)
            new_speed = (current_speed * 0.7) + (instant_speed * 0.3)
        else:
            # PenalizaciÃ³n fuerte si falla: cortamos su velocidad a la mitad
            new_speed = current_speed * 0.5 
            
        self.peer_performance[peer_id] = new_speed

    # ==========================================
    #               CLIENTE DE RED
    # ==========================================

    def request_chunk(self, peer_id, idx, file_hash, fm):
        # 1. SEGURIDAD: Verificar lista negra
        if peer_id in self.banned_peers:
            if time.time() < self.banned_peers[peer_id]: return "banned"
            else: del self.banned_peers[peer_id] 

        target_ip, target_port = peer_id.split(':')
        
        # En Bridged mode, usar 127.0.0.1 como fallback puede confundir si target_ip es la propia
        ips_to_try = [target_ip] 
        
        s = None
        for ip_cand in ips_to_try:
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.settimeout(5.0) 
                s.connect((ip_cand, int(target_port)))
                break
            except: 
                if s: s.close()
                s = None
        
        if not s: return "network_error"

        try:
            msg = {"command": CMD_REQUEST_CHUNK, "file_hash": file_hash, "chunk_index": idx}
            
            # --- CORRECCIÃ“N CRÃTICA: AÃ±adir \n al enviar ---
            s.sendall(json.dumps(msg).encode() + b'\n')
            
            f = s.makefile('rb')
            head_line = f.readline()
            if not head_line: return "network_error"
            
            head = json.loads(head_line)
            
            if head.get('status') == 'ok':
                expected_size = head['size']
                chunk_data = b""
                
                # Leer en bucle hasta completar el tamaÃ±o exacto (Crucial para chunks grandes)
                while len(chunk_data) < expected_size:
                    packet = f.read(expected_size - len(chunk_data))
                    if not packet: break
                    chunk_data += packet

                if len(chunk_data) != expected_size: return "network_error"

                # Hash Check
                expected = self.get_expected_hash(file_hash, idx)
                if expected:
                    calc = hashlib.sha256(chunk_data).hexdigest()
                    if calc != expected:
                        self.banned_peers[peer_id] = time.time() + 60
                        return "corrupt"

                fm.write_chunk(idx, chunk_data)
                return "success"
            
            elif head.get('status') == 'missing': return "missing"
        except: return "network_error"
        finally: 
            if s: s.close()
        return "network_error"





    def get_expected_hash(self, file_hash, idx):
        mgr = self.managers.get(file_hash)
        if not mgr: return None
        json_path = os.path.join(self.folder, f"{mgr['filename']}.json")
        try:
            # Nota: En producciÃ³n esto se deberÃ­a cachear en memoria
            with open(json_path, 'r') as f:
                data = json.load(f)
                return data['piece_hashes'][idx]
        except: return None

    # ==========================================
    #               SERVIDOR
    # ==========================================
    def start_server(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        s.bind(('0.0.0.0', self.my_port))
        s.listen(10)
        while self.running:
            try:
                conn, addr = s.accept()
                threading.Thread(target=self.handle_upload, args=(conn,)).start()
            except: pass




    def handle_upload(self, conn):
        try:
            conn.settimeout(30)
            # Imprimimos quiÃ©n se conecta para saber que la peticiÃ³n llegÃ³
            #print(f"[DEBUG SERVER] ConexiÃ³n entrante establecida...")
            
            raw = conn.recv(4096).decode()
            
            if not raw: 
                print("[DEBUG SERVER] RecibÃ­ 0 bytes (Cliente cerrÃ³ conexiÃ³n)")
                return
            
            #print(f"[DEBUG SERVER] Datos recibidos: {raw.strip()}") # Ver quÃ© llegÃ³ exacto
            
            req = json.loads(raw.strip())
            cmd = req.get('command')
            
            if cmd == CMD_REQUEST_CHUNK:
                mgr = self.managers.get(req.get('file_hash'))
                if mgr:
                    data = mgr['fm'].read_chunk(req.get('chunk_index'))
                    if data:
                        header = json.dumps({"status": "ok", "size": len(data)})
                        conn.sendall(header.encode() + b'\n' + data)
                        #print(f"[DEBUG SERVER] Chunk {req.get('chunk_index')} enviado.")
                    else:
                        conn.sendall(json.dumps({"status": "missing"}).encode() + b'\n')
                else:
                    conn.sendall(json.dumps({"status": "missing"}).encode() + b'\n')
            
            elif cmd == CMD_GET_METADATA:
                requested_hash = req.get('file_hash')
                print(f"[DEBUG SERVER] Solicitan metadatos para hash: {requested_hash}")
                
                mgr = self.managers.get(requested_hash)
                
                if mgr:
                    # Recuperar hashes si existen
                    try:
                        json_path = os.path.join(self.folder, f"{mgr['filename']}.json")
                        with open(json_path, 'r') as f:
                            saved_data = json.load(f)
                            hashes = saved_data.get('piece_hashes', [])
                    except: hashes = []

                    meta = {
                        "status": "ok", 
                        "filename": mgr['filename'], 
                        "filesize": mgr['fm'].total_size, 
                        "trackers": mgr['trackers'],
                        "piece_hashes": hashes 
                    }
                    conn.sendall(json.dumps(meta).encode() + b'\n')
                    print(f"[DEBUG SERVER] Metadatos enviados exitosamente.")
                else:
                    print(f"[DEBUG SERVER] ERROR: No tengo cargado el archivo con hash {requested_hash}")
                    error_msg = {"status": "error", "message": "File not hosted here"}
                    conn.sendall(json.dumps(error_msg).encode() + b'\n')

        except json.JSONDecodeError:
            print(f"[ERROR FATAL] RecibÃ­ basura que no es JSON vÃ¡lido: {raw}")
        except Exception as e:
            # AQUÃ ESTÃ LA CLAVE: Imprimir el error real con traceback
            print(f"[ERROR FATAL] ExcepciÃ³n en handle_upload: {e}")
            traceback.print_exc() 
        finally: 
            conn.close()
        
        

    # ==========================================
    #        GESTIÃ“N Y AUXILIARES
    # ==========================================
    def add_manager(self, filename, file_hash, size, trackers, force_leecher=False):
        with self.managers_lock:
            # Si ya existe en memoria, lo devolvemos (a menos que queramos recargar, 
            # pero eso lo manejamos borrÃ¡ndolo antes en search_tracker)
            if file_hash in self.managers: 
                return self.managers[file_hash]
            
            real_path = os.path.join(self.folder, filename)
            
            # LÃ“GICA CORREGIDA:
            # Es Seeder SOLO si el tamaÃ±o coincide Y no nos estÃ¡n forzando a ser leecher
            size_match = os.path.exists(real_path) and os.path.getsize(real_path) == size
            is_seeder = size_match and not force_leecher
            
            fm = FileManager(self.folder, filename, size, is_seeder)
            self.managers[file_hash] = {"fm": fm, "filename": filename, "trackers": trackers, "downloading": False}
            return self.managers[file_hash]

    def scan_folder(self):
        print("[*] Escaneando carpeta y verificando integridad...")
        json_files = glob.glob(os.path.join(self.folder, "*.json"))
        
        for json_path in json_files:
            try:
                with open(json_path, 'r') as f: meta = json.load(f)
                
                # Evitamos procesar si ya lo tenemos cargado en memoria
                with self.managers_lock:
                    if meta['filehash'] in self.managers: continue
                
                real_path = os.path.join(self.folder, meta['filename'])
                progress_file = real_path + ".progress"
                
                # --- VERIFICACIÃ“N REAL DE INTEGRIDAD ---
                if os.path.exists(real_path) and 'piece_hashes' in meta:
                    
                    # 1. Usar el tamaÃ±o de pieza del JSON (o el global si no existe)
                    p_size = meta.get('piece_size', BLOCK_SIZE)
                    
                    valid_chunks = []
                    total_chunks = len(meta['piece_hashes'])
                    
                    # 2. Leer el archivo fÃ­sico y comparar hashes
                    # Esto tarda unos segundos, pero garantiza que no mientas al tracker
                    with open(real_path, 'rb') as f_check:
                        for i, expected_hash in enumerate(meta['piece_hashes']):
                            f_check.seek(i * p_size)
                            chunk_data = f_check.read(p_size)
                            if not chunk_data: break
                            
                            if hashlib.sha256(chunk_data).hexdigest() == expected_hash:
                                valid_chunks.append(i)
                    
                    # 3. Determinar si es perfecto
                    is_perfect = (len(valid_chunks) == total_chunks)
                    
                    # 4. Si falta algo, actualizamos el archivo .progress para no perder el avance
                    if not is_perfect:
                        pct = (len(valid_chunks) / total_chunks) * 100
                        print(f"   âš ï¸  Recuperado parcial: {meta['filename']} ({pct:.1f}%)")
                        with open(progress_file, 'w') as f_prog:
                            json.dump({"downloaded": valid_chunks}, f_prog)
                    else:
                        print(f"   âœ… Archivo verificado 100%: {meta['filename']}")

                    # 5. Cargar al Manager (Si no es perfecto, forzamos modo Leecher)
                    self.add_manager(
                        meta['filename'], 
                        meta['filehash'], 
                        meta['filesize'], 
                        KNOWN_TRACKERS, 
                        force_leecher=(not is_perfect) 
                    )
                    
                    # 6. Anunciar estado real al Tracker
                    self.contact_tracker(CMD_ANNOUNCE, meta['filehash'], KNOWN_TRACKERS)

            except Exception as e: 
                print(f"[!] Error al escanear {json_path}: {e}")

    def fetch_metadata_from_swarm(self, file_hash, peers_list):
        print(f"[*] Buscando metadatos...")
        for p_data in peers_list:
            if p_data['id'] == self.my_id: continue 
            target_ip, target_port = p_data['id'].split(':')
            ips_to_try = [target_ip, '127.0.0.1']
            for ip_cand in ips_to_try:
                s = None
                try:
                    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    s.settimeout(2)
                    s.connect((ip_cand, int(target_port)))
                    
                    # CorrecciÃ³n importante: Enviar \n al final
                    msg = {"command": CMD_GET_METADATA, "file_hash": file_hash}
                    s.send(json.dumps(msg).encode() + b'\n') 
                    
                    f = s.makefile('rb')
                    line = f.readline()
                    if line: 
                        resp = json.loads(line)
                        s.close()
                        return resp
                    s.close()
                except: 
                    if s: s.close()
        return None






    def contact_tracker(self, command, file_hash=None, trackers=None):
        if not trackers: trackers = KNOWN_TRACKERS
        combined_peers = []
        combined_files = [] 
        success_count = 0
        
        # --- MEMORIA ANTI-SPAM ---
        # Iniciamos el set si no existe para recordar quÃ© archivos ya avisamos
        if not hasattr(self, 'announced_files'):
            self.announced_files = set()

        # Recorremos todos los trackers
        for ip, port in trackers:
            s = None
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.settimeout(5) # 5 segundos de espera mÃ¡ximo por tracker
                s.connect((ip, port))
                
                req = {"command": command, "peer_id": self.my_id}
                if file_hash:
                    req["file_hash"] = file_hash
                    mgr = self.managers.get(file_hash)
                    if mgr: req.update({"filename": mgr["filename"], "percent": mgr["fm"].get_progress_percentage()})
                
                s.send(json.dumps(req).encode())
                
                raw_resp = s.recv(BUFFER_SIZE).decode()
                if not raw_resp: continue
                
                resp = json.loads(raw_resp)
                
                if resp.get('status') == 'ok':
                    success_count += 1
                    
                    if command == CMD_LIST_FILES and 'files' in resp:
                        combined_files.extend(resp['files'])
                    
                    if command == CMD_ANNOUNCE and 'peers' in resp:
                        combined_peers.extend(resp['peers'])

            except:
                pass # Si falla uno, seguimos con el siguiente en silencio
            finally:
                if s: s.close()
        
        # --- MENSAJE AL USUARIO (Solo una vez por archivo) ---
        # Se ejecuta SOLO si es un Anuncio, si hubo Ã©xito, y si NO lo hemos dicho antes.
        if command == CMD_ANNOUNCE and file_hash and success_count > 0:
            if file_hash not in self.announced_files:
                mgr = self.managers.get(file_hash)
                fname = mgr['filename'] if mgr else "Archivo"
                print(f"[ğŸŒ] Enjambre conectado: {fname} (Visible en {success_count} trackers)")
                self.announced_files.add(file_hash)

        # Procesamiento de duplicados
        if command == CMD_LIST_FILES:
            seen_hashes = set()
            unique_files = []
            for f in combined_files:
                if f['hash'] not in seen_hashes:
                    unique_files.append(f)
                    seen_hashes.add(f['hash'])
            return unique_files
            
        if command == CMD_ANNOUNCE:
            seen = set()
            unique_peers = []
            for p in combined_peers:
                if p['id'] not in seen:
                    unique_peers.append(p)
                    seen.add(p['id'])
            return unique_peers
            
        return []




    def heartbeat_loop(self):
        while self.running:
            time.sleep(2)
            
            #self.scan_folder()
            
            with self.managers_lock: hashes = list(self.managers.keys())
            for h in hashes:
                mgr = self.managers.get(h)
                if not mgr['downloading']:
                    self.contact_tracker(CMD_ANNOUNCE, h, KNOWN_TRACKERS)

    # ==========================================
    #               INTERFAZ / MONITOR
    # ==========================================
    def monitor(self):
        try:
            while True:
                os.system('cls' if os.name == 'nt' else 'clear')
                print(f"\nâ”Œâ”€â”€ ESTADO DE ARCHIVOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                with self.managers_lock:
                    if not self.managers: print(" [Sin actividad]")
                    for h, m in self.managers.items():
                        pct = m['fm'].get_progress_percentage()
                        filled = int(pct / 5)
                        bar = "â–ˆ" * filled + "â–‘" * (20 - filled)
                        state = "BAJANDO â¬‡" if m['downloading'] else ("SEEDING â¬†" if pct==100 else "PAUSA â¸")
                        print(f" {m['filename'][:15]:<15} [{bar}] {pct:>3.0f}% {state}")
                
                print(f"\nâ”Œâ”€â”€ BALANCEO DE CARGA (QuiÃ©n me alimenta) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                print(f"â”‚ {'PEER ID':<20} | {'LATENCIA':<9} | {'PIEZAS':<8} â”‚")
                print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
                all_peers = set(list(self.peer_performance.keys()) + list(self.download_stats.keys()))
                if not all_peers: print(f"â”‚ {'(Esperando datos...)':<43} â”‚")
                for pid in all_peers:
                    lat = self.peer_performance.get(pid, 0.0)
                    chunks = self.download_stats.get(pid, 0)
                    print(f"â”‚ {pid:<20} | {lat:.4f}s  | {chunks:<8} â”‚")
                print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
                time.sleep(2.5)
        except KeyboardInterrupt: pass

    def sabotage_file(self):
        print("\n--- MODO SABOTAJE (CORRUPCIÃ“N) ---")
        target = input(">> Ruta del archivo a corromper: ").strip()
        
        if not os.path.exists(target):
            print("âŒ El archivo no existe.")
            return

        try:
            file_size = os.path.getsize(target)
            header_safe = 50000  # Protegemos los primeros 50KB (cabecera)
            damage_size = 2048   # DaÃ±amos 2KB
            
            if file_size < (header_safe + damage_size):
                print("âš ï¸ Archivo muy pequeÃ±o para corromper de forma segura.")
                return

            # Calcular punto aleatorio
            max_offset = file_size - damage_size
            random_offset = random.randint(header_safe, max_offset)

            # Inyectar basura
            with open(target, "r+b") as f:
                f.seek(random_offset)
                f.write(os.urandom(damage_size))
            
            print(f"ğŸ’€ Ã‰XITO: Se inyectÃ³ ruido en el byte {random_offset} del archivo.")
            print("   (El hash del archivo ha cambiado, pero la cabecera sigue intacta)")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            
            
            
    def verify_integrity(self, filepath, expected_hash):
        """
        Calcula el hash SHA-256 del archivo descargado y lo compara con el original.
        Retorna True si son idÃ©nticos, False si el archivo estÃ¡ corrupto.
        """
        print(f"ğŸ•µï¸ Verificando integridad de: {filepath}...")
        
        sha256_hash = hashlib.sha256()
        
        try:
            with open(filepath, "rb") as f:
                # Leemos por bloques para no saturar la RAM
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            
            calculated_hash = sha256_hash.hexdigest()
            
            if calculated_hash == expected_hash:
                print("âœ… INTEGRIDAD APROBADA. El archivo es autÃ©ntico.")
                return True
            else:
                print("ğŸš¨ ALERTA DE SEGURIDAD: HASH MISMATCH")
                print(f"   Esperado: {expected_hash}")
                print(f"   Calculado: {calculated_hash}")
                return False
                
        except Exception as e:
            print(f"âŒ Error al verificar archivo: {e}")
            return False



    def main_menu(self):
        while self.running:
            print(f"\n=== NODE {self.my_ip}:{self.my_port} ===")
            # Agregamos la opciÃ³n 5 visualmente
            print("1. Publicar | 2. Descargar | 3. Monitor | 4. Salir | 5. ğŸ’€ Sabotaje")
            
            op = input(">> ")
            
            if op == '1': self.convert_local_file()
            elif op == '2': self.search_tracker()
            elif op == '3': self.monitor()
            elif op == '4': 
                print("ğŸ‘‹ Cerrando sesiÃ³n en el Tracker...")
                # Avisar a todos los trackers que me voy
                # Enviamos 'None' como hash para indicar que me voy de TODOS los swarms
                self.contact_tracker(CMD_EXIT_SWARM, file_hash=None, trackers=KNOWN_TRACKERS)
                
                self.running = False
                sys.exit()
            # Agregamos la llamada lÃ³gica
            elif op == '5': self.sabotage_file()
            else: print("OpciÃ³n no vÃ¡lida")




    def convert_local_file(self):
        print("\n--- PUBLICAR ---")
        if not os.path.exists(self.folder): print("Carpeta no existe."); return
        
        # Filtramos para no mostrar jsons ni archivos de progreso
        all_f = [f for f in os.listdir(self.folder) if os.path.isfile(os.path.join(self.folder, f))]
        cands = [f for f in all_f if not f.endswith('.json') and not f.endswith('.progress') and f"{f}.json" not in all_f]
        
        if not cands: print("[!] No hay archivos nuevos."); return

        for i, f in enumerate(cands): print(f"{i+1}. {f}")
        try:
            sel = int(input(">> #: ")) - 1
            if 0 <= sel < len(cands):
                fname = cands[sel]
                fpath = os.path.join(self.folder, fname)
                size = os.path.getsize(fpath)
                
                print(f"[*] Generando hashes de seguridad (BLOCK_SIZE={BLOCK_SIZE})...")
                
                # --- LÃ“GICA INTEGRADA DE CREAR_TORRENT ---
                sha256_global = hashlib.sha256()
                piece_hashes = []
                
                with open(fpath, 'rb') as f:
                    while True:
                        chunk = f.read(BLOCK_SIZE)
                        if not chunk: break
                        
                        # 1. Hash Global
                        sha256_global.update(chunk)
                        
                        # 2. Hash por Pieza (CRÃTICO PARA SEGURIDAD)
                        p_hash = hashlib.sha256(chunk).hexdigest()
                        piece_hashes.append(p_hash)
                
                fhash = sha256_global.hexdigest()
                # -----------------------------------------

                meta = {
                    "filename": fname, 
                    "filesize": size, 
                    "filehash": fhash, 
                    "piece_hashes": piece_hashes, # Â¡Ahora sÃ­ incluimos la seguridad!
                    "trackers": KNOWN_TRACKERS
                }
                
                # Guardamos el .json
                with open(os.path.join(self.folder, f"{fname}.json"), 'w') as f: 
                    json.dump(meta, f, indent=4)
                
                # Nos registramos como dueÃ±os del archivo
                self.add_manager(fname, fhash, size, KNOWN_TRACKERS)
                
                # Avisamos al tracker
                self.contact_tracker(CMD_ANNOUNCE, fhash, KNOWN_TRACKERS)
                
                print(f"[OK] {fname} publicado con {len(piece_hashes)} piezas verificadas.")
        except Exception as e:
            print(f"[ERROR] Al publicar: {e}")



    def search_tracker(self):
        files = self.contact_tracker(CMD_LIST_FILES)
        if not files: print("[!] Tracker vacÃ­o."); return
        
        print(f"\n--- DISPONIBLES ---")
        for i, f in enumerate(files): print(f"{i+1}. {f['filename']} (Peers: {f['peers_count']})")
        
        try:
            sel = int(input(">> Descargar #: ")) - 1
            if 0 <= sel < len(files):
                target = files[sel]
                f_hash = target['hash']
                jpath = os.path.join(self.folder, f"{target['filename']}.json")
                
                # =======================================================
                #    LÃ“GICA MAESTRA DE RECUPERACIÃ“N (CORREGIDA)
                # =======================================================
                if os.path.exists(jpath):
                    try:
                        with open(jpath, 'r') as f: meta = json.load(f)
                        
                        if 'filehash' in meta:
                            print(f"[*] ConfiguraciÃ³n encontrada: {meta['filename']}")
                            local_file = os.path.join(self.folder, meta['filename'])
                            progress_file = local_file + ".progress"
                            
                            # LIMPIEZA DE MEMORIA CRÃTICA:
                            # Borramos cualquier gestor previo para obligar a recargar desde disco
                            with self.managers_lock:
                                if f_hash in self.managers: del self.managers[f_hash]

                            # CASO 1: Â¿Existe el archivo de video FÃSICAMENTE?
                            if os.path.exists(local_file) and 'piece_hashes' in meta:
                                
                                print(f"ğŸ•µï¸ El archivo existe. Escaneando integridad bloque a bloque...")
                                
                                # --- CORRECCIÃ“N 1: Usar el tamaÃ±o del JSON, no el global ---
                                json_piece_size = meta.get('piece_size', BLOCK_SIZE)
                                # -----------------------------------------------------------

                                valid_chunks = []
                                total_chunks = len(meta['piece_hashes'])
                                corrupted_count = 0
                                
                                with open(local_file, 'rb') as f_check:
                                    for i, expected_hash in enumerate(meta['piece_hashes']):
                                        # Usamos la variable local, no la constante global
                                        f_check.seek(i * json_piece_size)
                                        chunk_data = f_check.read(json_piece_size)
                                        
                                        if not chunk_data: break
                                        
                                        if hashlib.sha256(chunk_data).hexdigest() == expected_hash:
                                            valid_chunks.append(i)
                                        else:
                                            corrupted_count += 1
                                
                                # Reporte
                                if corrupted_count > 0:
                                    print(f"âš ï¸ Se encontraron {corrupted_count} piezas corruptas/modificadas.")
                                    print(f"âœ… Se rescataron {len(valid_chunks)} piezas vÃ¡lidas.")
                                elif len(valid_chunks) < total_chunks:
                                    print(f"â„¹ï¸ Archivo incompleto. Reanudando descarga...")
                                else:
                                    print(f"âœ… Archivo verificado al 100%. Listo para Seeding.")

                                # Actualizamos el .progress
                                with open(progress_file, 'w') as f_prog:
                                    json.dump({"downloaded": valid_chunks}, f_prog)
                                
                                # REINICIO CON FORCE_LEECHER
                                # Si hubo corrupciÃ³n o faltan piezas, pasamos force_leecher=True
                                is_imperfect = (corrupted_count > 0) or (len(valid_chunks) < total_chunks)
                                
                                self.add_manager(meta['filename'], meta['filehash'], meta['filesize'], KNOWN_TRACKERS, force_leecher=is_imperfect)
                                self.start_download_thread(meta['filehash'])
                                return

                            # CASO 2: Existe JSON pero NO video
                            else:
                                print("âš ï¸ JSON encontrado pero sin video. Reiniciando descarga.")
                                if os.path.exists(progress_file): os.remove(progress_file)
                                
                                self.add_manager(meta['filename'], meta['filehash'], meta['filesize'], KNOWN_TRACKERS)
                                self.start_download_thread(meta['filehash'])
                                return

                    except Exception as e: 
                        print(f"[!] Error leyendo configuraciÃ³n local: {e}")
                        traceback.print_exc() # Ãštil para ver el error real
                        if os.path.exists(jpath): os.remove(jpath)

                # ==========================================
                #  PASO 2: Descarga de red (Nuevo)
                # ==========================================
                peers = self.contact_tracker(CMD_ANNOUNCE, f_hash, KNOWN_TRACKERS)
                if not peers: print("[!] Sin peers."); return
                
                meta = self.fetch_metadata_from_swarm(f_hash, peers)
                if meta and 'filename' in meta:
                    save_meta = {
                        "filename": meta['filename'], 
                        "filesize": meta['filesize'], 
                        "filehash": f_hash, 
                        "trackers": KNOWN_TRACKERS,
                        "piece_hashes": meta.get('piece_hashes', []),
                        # --- CORRECCIÃ“N 2: Guardar el tamaÃ±o de pieza para el futuro ---
                        "piece_size": meta.get('piece_size', BLOCK_SIZE) 
                        # ---------------------------------------------------------------
                    }
                    with open(jpath, 'w') as f: json.dump(save_meta, f, indent=4)
                    
                    self.add_manager(meta['filename'], f_hash, meta['filesize'], KNOWN_TRACKERS)
                    self.start_download_thread(f_hash)
                    print("[OK] Metadatos obtenidos. Iniciando descarga...")
                else: 
                    print("[ERROR] No se pudieron obtener metadatos.")

        except Exception as e:
            traceback.print_exc()
            print(f"[CRASH] Error fatal: {e}")



if __name__ == "__main__":
    if len(sys.argv) < 2: 
        print("Uso: python3 peer_node.py <PUERTO_LOCAL>")
        sys.exit()
    
    ip_sugerida = get_local_ip()
    print("--- CONFIGURACIÃ“N ---")
    print(f"DetectÃ© que tu IP local es: {ip_sugerida}")
    tracker_hint = input("IP del Tracker Principal [Enter para usar tu IP local]: ")
    if not tracker_hint.strip(): tracker_hint = ip_sugerida
    
    PeerNode(int(sys.argv[1]), tracker_hint).main_menu()

